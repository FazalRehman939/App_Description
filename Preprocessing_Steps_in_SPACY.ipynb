{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## import the required libraries and packages ##\n",
    "import re\n",
    "import contractions\n",
    "import spacy\n",
    "import unidecode\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.attrs import IS_PUNCT, LOWER, POS\n",
    "# for Word tokenization import\n",
    "from spacy.lang.en import English\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp1 = spacy.load('en_core_web_sm', disable=['parser', 'ner']) #for lemmatization purpose\n",
    "\n",
    "class TextProcessing:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.nlp = spacy.load('en_core_web_sm')\n",
    "        self.sentence_filter = [\"www\", \"http\", \"https\", \".com\", \".de\"]\n",
    "        self.sentence_filter_quotes = [\"\\\"\", \"“\", \"”\"]\n",
    "        self.email_pattern = \"r'[\\w\\.-]+@[\\w\\.-]+'\"\n",
    "        self.stoplist = stopwords.words('english')\n",
    "        self.stoplist.append(\"own\") ## you can add custom stopwords by yourself ##\n",
    "        self.stoplist.append(\"%\")\n",
    "        self.stoplist.append(\"anyone\")\n",
    "        self.stoplist.append(\"want\")\n",
    "        self.stoplist.append(\"period\")\n",
    "        self.stoplist.remove('with') ## you can remove custom stopwords by yourself ##\n",
    "        self.stoplist.remove('more')\n",
    "        self.stoplist.remove('and')\n",
    "        self.stoplist.remove('or')\n",
    "        self.stoplist.remove('on')\n",
    "        self.stoplist=set(self.stoplist) ## making the stopword as set b/c to remove repitition ##\n",
    "#         self.stoplist = set(stopwords.words('english'))\n",
    "#         self.stoplist = {\"own\",\"%\"}\n",
    "\n",
    "    def process_text(self, text):  ## we are calling methods used in preprocessing steps from here ##\n",
    "        \n",
    "        text1 = self.unidecode(text)\n",
    "        text2 = self.remove_extra_whitespaces(text1)\n",
    "        text3 = self.expand_contraction(text2)\n",
    "        text4 = self.remove_stopwords(text3)\n",
    "        text5 = self.filter_sentences(text4)\n",
    "        text6 = self.lower_case(text5)\n",
    "        \n",
    "        return text6\n",
    "\n",
    "    \n",
    "    def filter_sentences(self, text):\n",
    "        # remove sentences that e.g. contain URLs because they are used to address the contact to the publisher\n",
    "        # remove sentences that contain email addresses as they are used for providing a contact to the publisher\n",
    "        text = re.sub('\\\"','',text) #remove qoutaion marks\n",
    "        text= re.sub(r'''(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]\n",
    "        +\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\".,<>?«»“”‘’]))''', \" \", text ,flags=re.MULTILINE)\n",
    "        #print(text)\n",
    "        return text\n",
    "    \n",
    "    \n",
    "    def expand_contraction(self, text):\n",
    "        \n",
    "        text=contractions.fix(text)\n",
    "        return text\n",
    "    \n",
    "    def remove_stopwords(self, text):\n",
    "        \n",
    "        text_without_sw = [word for word in text.split() if word.lower() not in self.stoplist]\n",
    "        return \" \".join(text_without_sw)\n",
    "        \n",
    "\n",
    "\n",
    "    def remove_extra_whitespaces(self, text):\n",
    "        \"\"\"\n",
    "        Removes extra whitespaces from the text\n",
    "        Args:\n",
    "            text: text to be processed\n",
    "        \"\"\"\n",
    "        return text.strip()\n",
    "    \n",
    "   \n",
    "    def unidecode(self, text):\n",
    "        \"\"\"\n",
    "        unidecodes the text\n",
    "        Args:\n",
    "            text: text to be processed\n",
    "        \"\"\"\n",
    "        return unidecode.unidecode(text.lower())\n",
    "\n",
    "    def lower_case(self, text):\n",
    "        \"\"\"\n",
    "        lower cases the text\n",
    "        Args:\n",
    "            text: text to be processed\n",
    "\n",
    "        \"\"\"\n",
    "        return text.lower()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
